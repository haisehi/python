{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Quang Huy\\AppData\\Local\\Temp\\ipykernel_13300\\3840418038.py:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  .config(\"spark.sql.warehouse.dir\",\"D:\\code\\python\\spark\\exercise\") \\\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.sql.warehouse.dir\",\"D:\\code\\python\\spark\\exercise\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Quang Huy\\AppData\\Local\\Temp\\ipykernel_13300\\1508527345.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  orders_df = spark.read.csv(\"D:\\code\\python\\spark\\exercise\\orders_wh.csv\", header=True, inferSchema=True)\n"
     ]
    }
   ],
   "source": [
    "orders_df = spark.read.csv(\"D:\\code\\python\\spark\\exercise\\orders_wh.csv\", header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bang tam la cau noi giua sparksql va dataframe\n",
    "orders_df.createOrReplaceTempView(\"orders_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tao mot databases\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS SPARK_SQL_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         namespace|\n",
      "+------------------+\n",
      "|           default|\n",
      "|spark_sql_database|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#managed table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS SPARK_SQL_DATABASE.ORDERS_MANAGED AS SELECT * FROM orders_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-----------+\n",
      "|         namespace|     tableName|isTemporary|\n",
      "+------------------+--------------+-----------+\n",
      "|spark_sql_database|orders_managed|      false|\n",
      "|                  |    orders_tmp|       true|\n",
      "+------------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in spark_sql_database\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from spark_sql_database.orders_managed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                               |comment|\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                     |NULL   |\n",
      "|order_date                  |timestamp                                                               |NULL   |\n",
      "|customer_id                 |int                                                                     |NULL   |\n",
      "|order_status                |string                                                                  |NULL   |\n",
      "|                            |                                                                        |       |\n",
      "|# Detailed Table Information|                                                                        |       |\n",
      "|Catalog                     |spark_catalog                                                           |       |\n",
      "|Database                    |spark_sql_database                                                      |       |\n",
      "|Table                       |orders_managed                                                          |       |\n",
      "|Owner                       |Quang Huy                                                               |       |\n",
      "|Created Time                |Thu May 02 13:48:33 ICT 2024                                            |       |\n",
      "|Last Access                 |UNKNOWN                                                                 |       |\n",
      "|Created By                  |Spark 3.5.1                                                             |       |\n",
      "|Type                        |MANAGED                                                                 |       |\n",
      "|Provider                    |hive                                                                    |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1714632514]                                      |       |\n",
      "|Statistics                  |2862178 bytes                                                           |       |\n",
      "|Location                    |file:/D:/code/python/spark/exercise/spark_sql_database.db/orders_managed|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat              |       |\n",
      "|Storage Properties          |[serialization.format=1]                                                |       |\n",
      "|Partition Provider          |Catalog                                                                 |       |\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended spark_sql_database.orders_managed\").show(30,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#external table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
